```
台大資工陳縕儂老師的「深度學習之應用」課程影片：
0: Course Introduction 課程介紹與規定
1.1: What is ML? 甚麼是機器學習?
1.2: What is DL? 甚麼是深度學習?
1.3: How to Apply? 如何應用深度學習?
2.1: How to Train a Model? 如何訓練模型?
2.2: What is a Model? 模型是甚麼?
2.3: What does the "Good" Function Mean? 什麼叫做好的Function呢?
2.4: How can we Pick the "Best" Function? 如何找出最好的Function
2.5: Backpropagation 效率地計算大量參數
3.1: Word Representations 用機器看得懂的方式表示詞彙
3.2: Language Modeling 語言模型
3.3: Recurrent Neural Network 詳細解析 (20/03/17)
3.4: RNN Applications RNN各式應用 (20/03/17)
Vivian NTU MiuLab
2022 Fall Homework 1
Vivian NTU MiuLab
直播 QA (2022/09/22)
4: Gating Mechanism 了解LSTM與GRU的細節
5.1: Word Embeddings - Word Representation Review
5.2: Word2Vec 詞嵌入向量
5.3: Word2Vec Training 如何訓練詞嵌入向量
5.4: Word2Vec Variants 不同的詞嵌入變形
5.5: GloVe
5.6: Word Vector Evaluation 詞向量評估
6.1: Contextualized Word Embeddings 前後文相關之詞嵌入
6.2: ELMo 芝麻街家族的起源
7.1: Attention Mechanism 注意力機制
7.2: Attention Applications 注意力的各種應用
直播 QA (2022/10/06)
8.1: Self-Attention 自我注意力機制
8.2: Multi-Head Attention 不同類型的注意力關係
8.3 Transformer 稱霸各式AI任務之模型
9: Subword Tokenization 詳解BPE (Byte-Pair Encoding)
10.1 BERT 進擊的芝麻街巨人
QA 直播 (2022/10/13)
ADL 2022 Fall Homework 2
ADL 11.1: Transformer-XL 可吃超長文的變形金剛
11.2: XLNet 兼顧AE與AR好處的BERT
11.3: RoBERTa & SpanBERT 資料更多更好效能的BERT
11.4: Multilingual BERT & XLM 懂多國語言的BERT
直播 QA (2022/10/20)
11.1: Deep Reinforcement Learning Introduction (21/05/10)
9.2: Reinforcement Learning Introduction (21/05/10)
9.3: Reinforcement Learning (21/05/10)
9.4: Value-Based RL (21/05/10)
9.5: Q-Learning (21/05/10)
9.6: Advanced DQN (21/05/17)
10.1: Policy Gradient (21/05/17)
10.2: Actor-Critic (21/05/17)
13.1: Natural Language Generation
13.2: Decoding Algorithms 生成語言時須考量的策略
13.3: NLG Evaluation 目前研究上最大難題
13.4: RL for NLG 進一步提升語言生成效果
14.1: Model Pre-Training 預訓練模型介紹 (GPT, GPT-2)
14.2: GPT-3 現今最強預訓練語言模型
14.3: GPT-3 Alternatives: GPT-J, BLOOM, OPT GPT-3的替代方案
14.4: MS DialoGPT, Google LaMDA 專門生成對話的語言模型解析
14.5: BART, mBART, T5, mT5 利用denoise進行預訓練
14.6: Google Meena, Meta BlenderBot 比你還會對話的預訓練模型
15.1: Issues of PLMs 如何提示預訓練模型
15.2: (Hard) Prompt-Tuning, LM-BFF 用自然語言提示模型
15.3: (Soft) Prompt-Tuning (P-Tuning, Prefix Tuning) 人不懂沒關係機器懂就好
15.4: Instruction Tuning 讓機器了解任務指令
15.5: Prompting Paradigm 基於提示的研究大補帖
2022 Fall Final Project Introduction
16.1: Beyond Supervised Learning
16.2: Auto-Encoder 學習內部特徵以重建輸入
16.3: Variational Auto-Encoder (VAE) 固定特徵的分布資訊
16.4: Dual Learning 利用兩個任務的對稱性同時學習
16.5: Self-Supervised Learning (Self Prediction + Contrastive Learning) 自監督式學習
2022 Fall Final Project
Lecture 直播 (2022/12/08)
17.1: Issues in Pre-Trained Models 預訓練模型的問題
17.2: OpenAI InstructGPT 從人類回饋中學習 ChatGPT 的前身
17.3: OpenAI ChatGPT 驚驗眾人的對話互動式AI
17.4: OpenAI WebGPT 會搜尋找證據的GPT-3
18.1: CLIP Contrastive Language-Image Pretraining 透過語言與影像同時作預訓練
18.2: DALLE 2 Text-Guided Image Generation 利用CLIP特徵實現影像生成
```
* [Youtube](https://www.youtube.com/playlist?list=PLOAQYZPRn2V5yumEV1Wa4JvRiDluf83vn)
