{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "冒險25_打造自己的 Tokenizer (文字型資料的處理)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/csu1112-class/blob/main/class/%E5%86%92%E9%9A%AA25_%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84Tokenizer(%E6%96%87%E5%AD%97%E5%9E%8B%E8%B3%87%E6%96%99%E7%9A%84%E8%99%95%E7%90%86).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Zwoi-DYDR-g"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxc6kSls4X4t"
      },
      "source": [
        "### 2. 讀入套件\n",
        "\n",
        "我們最主要是需要 `tf.keras` 中的 `Tokenizer`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyV0Igeq5-OE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/yenlung/Python-AI-Book/main/dream.txt \\\n",
        "    -O /content/dream.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSJ-KgLazFVW",
        "outputId": "8a3293f0-ce78-4d33-e656-51922c5e3476"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-27 07:20:59--  https://raw.githubusercontent.com/yenlung/Python-AI-Book/main/dream.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2656053 (2.5M) [text/plain]\n",
            "Saving to: ‘/content/dream.txt’\n",
            "\n",
            "/content/dream.txt  100%[===================>]   2.53M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-05-27 07:21:00 (38.3 MB/s) - ‘/content/dream.txt’ saved [2656053/2656053]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 讀入蒐集到的所有文本"
      ],
      "metadata": {
        "id": "Fo0KP2wz1c2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('dream.txt', 'r')\n",
        "lines = f.readlines()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "myC1Bfdx1crV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lines = [x.lstrip('\\u3000\\u3000') for x in lines]"
      ],
      "metadata": {
        "id": "AmHGTXJE1akG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''.join(text_lines)"
      ],
      "metadata": {
        "id": "tOjCQNQcz7Ia"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 打造自己的 Tokenizer"
      ],
      "metadata": {
        "id": "_vFZp5Z411AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(char_level=True)"
      ],
      "metadata": {
        "id": "3LCDyAFP1zSW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "2b1yPEb415SH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences([\"我打造了一個函數學習機。\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ4CtB752IZO",
        "outputId": "877fd4ff-eab5-40df-9462-516d152ca2d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[15, 99, 721, 3, 6, 26, 597, 362, 1061, 912, 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts([[15, 99, 721, 3, 6, 26, 597, 362, 1061, 912, 2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgFlpIhg2Jmo",
        "outputId": "62efc0d5-8124-422b-dad1-d37a0c7de3fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['我 打 造 了 一 個 數 學 習 機 。']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. 把Tokenizer存起來"
      ],
      "metadata": {
        "id": "Z5Uq6JYD3X2C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUh9zFEFi2F",
        "outputId": "c6a2e41d-7b60-44cf-9103-5ae13c323828"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_4lNrxlF_M_",
        "outputId": "c8e8dba7-0345-4017-cc9c-a5582ad7b21f"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igw9qiUSGM77"
      },
      "source": [
        "f = open('MyTokenizer.pkl', 'wb')\n",
        "pickle.dump(tokenizer, f)\n",
        "f.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "之後要用 `pickle` 讀回我們訓練好的 tokenizer 是這樣:\n",
        "\n",
        "```python\n",
        "f = open('tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(f)\n",
        "f.close()\n",
        "```"
      ],
      "metadata": {
        "id": "4pkAc8tD3m3j"
      }
    }
  ]
}