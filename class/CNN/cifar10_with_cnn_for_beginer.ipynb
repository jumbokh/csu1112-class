{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/csu1112-class/blob/main/class/CNN/cifar10_with_cnn_for_beginer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An experimentation of computer vision challenge for beginer(75% val_accuracy in 25 epochs, and 79% after 50 epochs without data augmentation). \n",
        "\n",
        "The codes of this notebook are taken on keras documentation. I am just crying to give some explaination for that code.\n",
        "I hope that this migth be helpful for you.\n",
        "\n",
        "### Table of interest:\n",
        "1. Introduction\n",
        "2. Import and Preprocess the data\n",
        " + 2.1 Import all required libraries\n",
        " + 2.2 Import and preproces of data\n",
        " + 2.3 Distribution of data.\n",
        "3. Defining the model architecture Using ConVnets\n",
        "4. Model training\n",
        "5. Evaluate the model\n",
        " + 5.1 Training and validations cuvre\n",
        " + 5.2 Score trained model and prediction.\n",
        " + 5.3 Confusion matrix.\n",
        " + 5.4 Classification report.\n",
        " + 5.5 Check for the predictions.\n",
        "6. Save model and weights"
      ],
      "metadata": {
        "id": "lkhyZ1fOTDaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction.\n",
        "The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided into 10\n",
        "classes. Each class contains 6,000 images. The training set contains 50,000 images, while the test sets\n",
        "provides 10,000 images. This image taken from the CIFAR repository ( <a href = \"https://www.cs.toronto.edu/~kriz/cifar.html\">https://www.cs.toronto.edu/~kriz/cifar.html </a>). This is a classification problem with 10 classes(muti-label classification). We can take a view on this image for more comprehension of the dataset. \n",
        "\n",
        "![cifar10.png](attachment:cifar10.png)\n",
        "\n",
        "\n",
        "The challenge is to recognize previously unseen images and assign them to one of the 10 classes.\n",
        "\n",
        "Ok Let's get started."
      ],
      "metadata": {
        "id": "CuBOCumSTDaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import and Preprocess the data\n",
        "\n",
        "### 2.1 Import all required libraries"
      ],
      "metadata": {
        "id": "nuZEJcUXTDaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-wy496KdTDaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's setting the models hyperparameters and others global parameters."
      ],
      "metadata": {
        "id": "vcnnCT_XTDaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32  # The default batch size of keras.\n",
        "num_classes = 10  # Number of class for the dataset\n",
        "epochs = 100\n",
        "data_augmentation = False"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "JCVWs6YNTDag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Import and preproces of data \n",
        "We load the data and split it between train and test sets\n"
      ],
      "metadata": {
        "id": "5f7T7OK1TDah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-F_-VZC4TDaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Distribution of data."
      ],
      "metadata": {
        "id": "KoV35vkYTDal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "# Count plot for training set\n",
        "sns.countplot(y_train.ravel(), ax=axs[0])\n",
        "axs[0].set_title('Distribution of training data')\n",
        "axs[0].set_xlabel('Classes')\n",
        "# Count plot for testing set\n",
        "sns.countplot(y_test.ravel(), ax=axs[1])\n",
        "axs[1].set_title('Distribution of Testing data')\n",
        "axs[1].set_xlabel('Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KpC3d2zaTDam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, each classe contain exacly 6000 examples( 5000 for training and 1000 for test).\n",
        "\n",
        "The graph above is very important for the training, for example if we had just 1000 samples of label 1 that will be a problem , the model will find difficulties to detect label 1\"less accuracy \", so that's not going to happend everything look fine. It's important to know the distribution of dataset behind different classes because the goodness of our model depend on it.\n",
        "\n",
        "Now let's doing some preprocessing.\n",
        "\n",
        "The output variable have 10 posible values. This is a multiclass classification problem. We need to encode these lables to one hot vectors (ex : \"bird\" -> [0,0,1,0,0,0,0,0,0,0]). "
      ],
      "metadata": {
        "id": "yAimy4FuTDan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data. Before we need to connvert data type to float for computation.\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zAcOuFreTDap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Defining the model architecture Using ConVnets\n",
        "\n",
        "Now Let us define a suitable deep net.\n",
        "\n",
        "* In the first stage, Our net will learn **32 convolutional filters**, each of which with a **3 x 3 size**. The output dimension is the same one of the input shape, so it will be **32 x 32** and activation is `relu`, which is a simple way of introducing non-linearity; folowed by another **32 convolutional filters**, each of which with a **3 x 3 size** and activation is also `relu`. After that we have a **max-pooling** operation with `pool size` **2 x 2** and a `dropout` at **25%.**\n",
        "* In the next stage in the deep pipeline, Our net will learn **64 convolutional filters**, each of which with a **3 x 3 size**. The output dimension is the same one of the input shape and activation is `relu`; folowed by another **64 convolutional filters**, each of which with a **3 x 3 size** and activation is also `relu`. After that we have a **max-pooling** operation with `pool size` **2 x 2** and a `dropout` at **25%.**\n",
        "* And the Final stage in the deep pipeline is a dense network with **512 units** and `relu` activation followed by a `dropout` at **50%** and by a `softmax` layer with **10 classes as output**, one for each category.\n",
        "\n",
        "Now let us look at the code review for our architecture."
      ],
      "metadata": {
        "id": "k_QX6CgWTDaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the convnet\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# a softmax classifier\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "PvCuoAtATDar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, let us train the model.\n",
        "\n",
        "## 4. Model training\n",
        "\n",
        "Before making network ready for training we have to make sure to add below things:\n",
        "*   **A loss function:** to measure how good the network is\n",
        "*   **An optimizer:** to update network as it sees more data and reduce loss value\n",
        "*   **Metrics:** to monitor performance of network\n",
        "\n",
        "**Also note that for data augmentation:**\n",
        "* One of the most commun tehnique to avoid overfitting is data augmentation. And We know that overfitting is generaly occur when we don't have enought data for training the model. To avoid this overfitting problem, we need to expand artificially our dataset. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit. \n",
        "\n",
        "* Different data aumentation techniques are as follows: Cropping, Rotating, Scaling, Translating, Flipping, Adding Gaussian noise to input images, etc...\n"
      ],
      "metadata": {
        "id": "Ix71hSzOTDas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "EcfLXyvpTDat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = None  # For recording the history of trainning process.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                    batch_size=batch_size),\n",
        "                                    epochs=epochs,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    workers=4)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "hknbmjFtTDau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluate the model.\n",
        "\n",
        "### 5.1 Training and validation curves.\n",
        "Let's see the training and validation process by the visualization of history of fitting. This allow us to quickly know if how our model fit our data **(overfitting, underfitting, model convergence, etc...)**"
      ],
      "metadata": {
        "id": "jq8iOOBzTDaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "plotmodelhistory(history)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vEnlF7VbTDaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, after 60 epochs, the accuracy of our model doesn't really increase. But our model doesn't overffit.\n",
        "### 5.2 Score trained model and prediction."
      ],
      "metadata": {
        "id": "la34QF-qTDay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "# make prediction.\n",
        "pred = model.predict(x_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7gvI0w0CTDay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Let's investigate for errors.\n",
        "### 5.3 Confusion matrix.\n",
        "Confusion matrix can be very helpfull to see your model drawbacks.\n",
        "We plot the confusion matrix of the validation results.\n",
        "For good vizualization of our confusion matrix, we have to define to fonction."
      ],
      "metadata": {
        "id": "v3WrPTM4TDaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
        "    \"\"\"\n",
        "    Create a heatmap from a numpy array and two lists of labels.\n",
        "    \"\"\"\n",
        "    if not ax:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    # Plot the heatmap\n",
        "    im = ax.imshow(data, **kwargs)\n",
        "\n",
        "    # Create colorbar\n",
        "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
        "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # Let the horizontal axes labeling appear on top.\n",
        "    ax.tick_params(top=True, bottom=False,\n",
        "                   labeltop=True, labelbottom=False)\n",
        "    # We want to show all ticks...\n",
        "    ax.set_xticks(np.arange(data.shape[1]))\n",
        "    ax.set_yticks(np.arange(data.shape[0]))\n",
        "    # ... and label them with the respective list entries.\n",
        "    ax.set_xticklabels(col_labels)\n",
        "    ax.set_yticklabels(row_labels)\n",
        "    \n",
        "    ax.set_xlabel('Predicted Label') \n",
        "    ax.set_ylabel('True Label')\n",
        "    \n",
        "    return im, cbar\n",
        "\n",
        "def annotate_heatmap(im, data=None, fmt=\"d\", threshold=None):\n",
        "    \"\"\"\n",
        "    A function to annotate a heatmap.\n",
        "    \"\"\"\n",
        "    # Change the text's color depending on the data.\n",
        "    texts = []\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            text = im.axes.text(j, i, format(data[i, j], fmt), horizontalalignment=\"center\",\n",
        "                                 color=\"white\" if data[i, j] > thresh else \"black\")\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts"
      ],
      "metadata": {
        "trusted": true,
        "id": "5LjoweCZTDa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(pred, axis=1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test, axis=1)\n",
        "# Errors are difference between predicted labels and true labels\n",
        "errors = (Y_pred_classes - Y_true != 0)\n",
        "\n",
        "Y_pred_classes_errors = Y_pred_classes[errors]\n",
        "Y_pred_errors = pred[errors]\n",
        "Y_true_errors = Y_true[errors]\n",
        "X_test_errors = x_test[errors]\n",
        "\n",
        "cm = confusion_matrix(Y_true, Y_pred_classes) \n",
        "thresh = cm.max() / 2.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "im, cbar = heatmap(cm, labels, labels, ax=ax,\n",
        "                   cmap=plt.cm.Blues, cbarlabel=\"count of predictions\")\n",
        "texts = annotate_heatmap(im, data=cm, threshold=thresh)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_DMasN4lTDa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Classification report\n",
        "\n",
        "This will allow us to evaluate the model with other metrics **(Precision, Recall, F1 score, etc...)**"
      ],
      "metadata": {
        "id": "aq4ptJg5TDa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Y_true, Y_pred_classes))"
      ],
      "metadata": {
        "trusted": true,
        "id": "_Op1RI_tTDa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Check the predictions."
      ],
      "metadata": {
        "id": "hs0lBDWATDa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = 5\n",
        "C = 5\n",
        "fig, axes = plt.subplots(R, C, figsize=(12,12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in np.arange(0, R*C):\n",
        "    axes[i].imshow(x_test[i])\n",
        "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (labels[Y_true[i]], labels[Y_pred_classes[i]]))\n",
        "    axes[i].axis('off')\n",
        "    plt.subplots_adjust(wspace=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "AVw9PWk8TDa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Check the wrong predictions."
      ],
      "metadata": {
        "id": "JfZpqNYmTDa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = 3\n",
        "C = 5\n",
        "fig, axes = plt.subplots(R, C, figsize=(12,8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "misclassified_idx = np.where(Y_pred_classes != Y_true)[0]\n",
        "for i in np.arange(0, R*C):\n",
        "    axes[i].imshow(x_test[misclassified_idx[i]])\n",
        "    axes[i].set_title(\"True: %s \\nPredicted: %s\" % (labels[Y_true[misclassified_idx[i]]], \n",
        "                                                  labels[Y_pred_classes[misclassified_idx[i]]]))\n",
        "    axes[i].axis('off')\n",
        "    plt.subplots_adjust(wspace=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kEarfQhNTDa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Check the most important errors."
      ],
      "metadata": {
        "id": "JPVCpvtcTDa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n",
        "    \"\"\" This function shows 10 images with their predicted and real labels\"\"\"\n",
        "    n = 0\n",
        "    nrows = 2\n",
        "    ncols = 5\n",
        "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(12,6))\n",
        "    for row in range(nrows):\n",
        "        for col in range(ncols):\n",
        "            error = errors_index[n]\n",
        "            ax[row,col].imshow((img_errors[error]).reshape((32,32,3)))\n",
        "            ax[row,col].set_title(\"Predicted:{}\\nTrue:{}\".\n",
        "                                  format(labels[pred_errors[error]],labels[obs_errors[error]]))\n",
        "            n += 1\n",
        "            ax[row,col].axis('off')\n",
        "            plt.subplots_adjust(wspace=1)\n",
        "\n",
        "# Probabilities of the wrong predicted numbers\n",
        "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
        "\n",
        "# Predicted probabilities of the true values in the error set\n",
        "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
        "\n",
        "# Difference between the probability of the predicted label and the true label\n",
        "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
        "\n",
        "# Sorted list of the delta prob errors\n",
        "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
        "\n",
        "# Top 10 errors \n",
        "most_important_errors = sorted_dela_errors[-10:]\n",
        "\n",
        "# Show the top 10 errors\n",
        "display_errors(most_important_errors, X_test_errors, Y_pred_classes_errors, Y_true_errors)"
      ],
      "metadata": {
        "trusted": true,
        "id": "92zkZ5OTTDa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### - Testing the model with the test images in the test set.\n",
        "Now we can play with our model for some fun."
      ],
      "metadata": {
        "id": "brlAxmdITDa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_test(number):\n",
        "    fig = plt.figure(figsize = (3,3))\n",
        "    test_image = np.expand_dims(x_test[number], axis=0)\n",
        "    test_result = model.predict_classes(test_image)\n",
        "    plt.imshow(x_test[number])\n",
        "    dict_key = test_result[0]\n",
        "    plt.title(\"Predicted: {} \\nTrue Label: {}\".format(labels[dict_key],\n",
        "                                                      labels[Y_true[number]]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Li4jpxzJTDa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_test(20)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yFY-hxe5TDbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Save model and weights\n",
        "\n",
        "Note that we need to firstly indicate the directory to save the model and the name of our model. "
      ],
      "metadata": {
        "id": "asU5ewOCTDbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "metadata": {
        "_cell_guid": "",
        "_uuid": "",
        "trusted": true,
        "id": "LM6sn6ptTDbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Hope that you found this notebook helpful for you. More to come.\n",
        "\n",
        "### Thanks for sharing it and for your suggestions.\n"
      ],
      "metadata": {
        "id": "g1YcnQSFTDbD"
      }
    }
  ]
}