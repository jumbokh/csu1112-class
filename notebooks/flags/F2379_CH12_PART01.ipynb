{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/csu1112-class/blob/main/notebooks/flags/F2379_CH12_PART01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFL7wtclRDKe"
      },
      "source": [
        "#第12章：生成式深度學習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nl_lgB8RDKl"
      },
      "source": [
        "##12-1 使用LSTM來生成文字資料"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y65yDNrWRDKm"
      },
      "source": [
        "###12-1-3 取樣策略的重要性"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjrfDqc_RDKn"
      },
      "source": [
        "####程式 12.1 針對不同的temperature設定，重新加權並計算新的機率分佈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmid32zpRDKo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hutk9d-RDKp"
      },
      "source": [
        "###12-1-4 實作文字資料生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNuD7QrIRDKq"
      },
      "source": [
        "####程式 12.2 下載並解壓縮IMDB影評資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhkRqokuRDKq",
        "outputId": "001ea792-950a-4520-a77f-e05d42a76439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 07:14:03--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  19.2MB/s    in 7.1s    \n",
            "\n",
            "2022-05-25 07:14:11 (11.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc3sh2DGRDKr"
      },
      "source": [
        "####程式 12.3 使用文字檔來創建dataset物件（一個檔案為一筆樣本）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bv8TDaARDKr",
        "outputId": "1cc4e318-910f-485b-e603-cd89c86a62f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iNrHAa5RDKr"
      },
      "source": [
        "####程式 12.4 準備一個TextVectorization層"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhGCAPBZRDKs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzg7YSZBRDKs"
      },
      "source": [
        "####程式 12.5 創建資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33Kq5EA5RDKt"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfvr8Hb3RDKt"
      },
      "source": [
        "####重建第11章的Transformer區塊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1DHzLs5RDKu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAH97HjwRDKw"
      },
      "source": [
        "####程式 12.6 簡易的Transformer語言模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuhyfZQSRDKx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoF8ijyURDKx"
      },
      "source": [
        "###12-1-5 加入「可使用多種temperature來生成文字」的回呼(callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugIM7iJXRDKy"
      },
      "source": [
        "####程式 12.7 建立文字生成的callback物件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvF4rvcJRDKy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,\n",
        "                 generate_length,\n",
        "                 model_input_length,\n",
        "                 temperatures=(1.,),\n",
        "                 print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            print(\"== Generating with temperature\", temperature)\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(predictions[0, i, :])\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w512CNCRDKz"
      },
      "source": [
        "####程式 12.8 訓練語言模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apJajOmFRDK0",
        "outputId": "7dc715be-91c4-495d-d4d3-6f8f319a3318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "391/391 [==============================] - ETA: 0s - loss: 5.3566== Generating with temperature 0.2\n",
            "This movie movie mad is from for the tcm scenes comedy of long a features multiple a details compelling but and it not was fall wrongly in interesting 1997 but its craft fair and but on soon earth at though the the worst air universal force were it very belongs well to\n",
            "== Generating with temperature 0.5\n",
            "This movie is hell expecting just a too funny much excuse better for way laughs of should sound have makes been this captured movie like on seen the everything plain but except the other story [UNK] and [UNK] that the size effects that and comes can by show the [UNK] birds and\n",
            "== Generating with temperature 0.7\n",
            "This movie is is no a good romantic experience scifi with thriller whiny about [UNK] a and head thus straight watching movie it deals a with four [UNK] thousand and different james film [UNK] [UNK] [UNK] through camp some from [UNK] long [UNK] coleman is vision only of wholly ignorant [UNK] population\n",
            "== Generating with temperature 1.0\n",
            "This movie movie is i beyond only familiar good with movie three classic screwed tales of the the dvd backdrop and to who the was camera the ahead acting and from the the characters translation in [UNK] the the photography reverse by as robert poorly ryan constructed causes a george little [UNK]\n",
            "== Generating with temperature 1.5\n",
            "This movie is for set for in an the hour original of the time [UNK] making bonding a on [UNK] the country [UNK] character cards in and the [UNK] 45 in minutes a namely movie a the [UNK] women to with [UNK] the around even her worse luck with [UNK] his it\n",
            "391/391 [==============================] - 162s 398ms/step - loss: 5.3566\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.8163== Generating with temperature 0.2\n",
            "This movie film lacked was if great [UNK] acting drugs [UNK] emotion and in confusion this because movie the was director reasonable down but making had it himself looked to at make the worse exactly than what town makes this the movie movie people version boy of looked teaching up the as\n",
            "== Generating with temperature 0.5\n",
            "This movie is had awful the sex dialogue scenes and get profanity me absolutely wrong disgusting with aamir each khan other plans [UNK] the vincent special crawl effects and to twisted stay pieces at of that substitute saying [UNK] hammer sitting cat around next [UNK] to every the one spree of in\n",
            "== Generating with temperature 0.7\n",
            "This movie movie was made horrible with and this [UNK] movie house sucked [UNK] in talking fact what thats if about hed this ever movie spoken is words a more addict fun the paulie ending [UNK] says and nothing her what reactions happens about at 20 them years and after the his\n",
            "== Generating with temperature 1.0\n",
            "This movie movie doesnt was even i good dont tribes expect an this entertaining one beginning ever to made see a how movie the or mummy it barrels is the now best fox of sitcom all proportions seasons i are think a its chance just afterwards good a even show though about\n",
            "== Generating with temperature 1.5\n",
            "This movie film was is shot wonderful poorly it executed takes the place same the production federal perspective soldiers in in peoples national lives institution the noises story that follows demonstrated connery by meanwhile a killers day playing [UNK] a and former faced teenager by charles hatch steve a jagger [UNK] who\n",
            "391/391 [==============================] - 159s 406ms/step - loss: 4.8163\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.6013== Generating with temperature 0.2\n",
            "This movie is is very a bad writer movie and and director its are easy so to hard forget make grant him donald seriously sutherland trying is to flaws spectacle in alexandra the joseph role knox where myself he a must girlfriend follow is strong charmingly proverbial annoying fat characters conflicting dan\n",
            "== Generating with temperature 0.5\n",
            "This movie horrible had horrible no sound music the the music english is [UNK] no during attention this didnt event make until me later at the her end words when did things director in passes person in making a little television arranged movie accident that is is not handled famous with in\n",
            "== Generating with temperature 0.7\n",
            "This movie is was the poorly vampire filmed rapist just but acting ends that up utterly writing horribly the bad colour acting talked god about damn the the lister worst to film live ive action ever standing seen in better which ass this is film definitely the worth use seeing of it\n",
            "== Generating with temperature 1.0\n",
            "This movie is is kind funny of but a it guy starts master off he to wants tell his a mom father that is i no grew need tired these of characters any appearing decent thrown love in interest little by deja moral vu issues nick the is camera [UNK] work real\n",
            "== Generating with temperature 1.5\n",
            "This movie review is is irritated absolutely by terrible artistically acting tied that to instead a of shallow attempts characters at hey me thats why full i of know course its i like know heartwarming where sweet an and unpleasant asylum fernando and [UNK] euro rent horror back classic this hood movie\n",
            "391/391 [==============================] - 160s 407ms/step - loss: 4.6013\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.4767== Generating with temperature 0.2\n",
            "This movie lowkey is i so enjoyed doubt every the character acting he style is is fantastic completely as profound fancy and as acted he my and heart my [UNK] eyes you the are movie the is young not denied only the because presence director of because this [UNK] movie is rings\n",
            "== Generating with temperature 0.5\n",
            "This movie family is [UNK] pretty some bad okay for how those on minutes tv watch ever it as 1929 a which dare is i for have special seen effects at by craft different and i generally got like here helping im others concerned carlyle after is they a are let retired\n",
            "== Generating with temperature 0.7\n",
            "This movie is has an a interesting lot piece of of people folks any have claim winds to of reality mob with these romantic romances theme and and some [UNK] dopey are girl reduced book to this a [UNK] very delightful talented humor actor these john are [UNK] not farm just though\n",
            "== Generating with temperature 1.0\n",
            "This movie is was fantastic great i too think believe it it was isnt really good so for ill [UNK] attending and as randolph nicely i sang know that it i is feel still right truly it for about me her i now believe just first because saw parts this a before\n",
            "== Generating with temperature 1.5\n",
            "This movie is gets so a bad few while times it i looks also like liked great the music over bugs much me like as i were would sitting give next this break one i was would bored agree of in my the wrote neighborhood i ride owned as the i police\n",
            "391/391 [==============================] - 162s 414ms/step - loss: 4.4767\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.3899== Generating with temperature 0.2\n",
            "This movie tv is is mentioned quite in frankly point one the thing winner is didnt the stop chance building lets it see heart steiger wrenching has her all dying the great way miniseries she was met able with to a notice man but and it i becomes assumed even that cynical\n",
            "== Generating with temperature 0.5\n",
            "This movie is is a shocking fantastic film sci yet fi oddly can francis enjoy ford which emmanuelle is [UNK] so less much celebrated going queen for of anna silver christie screen though time a and little some research guy secret who hmm walk appears out saying by its todays because [UNK]\n",
            "== Generating with temperature 0.7\n",
            "This movie is wasnt terrible bad as enough it to is be known eva on [UNK] [UNK] channing sinclair tatum makes useless him go even wrong though patty hes she very occasionally nice acts which red is dwarf never in the his film face moves suit are is way really too shes\n",
            "== Generating with temperature 1.0\n",
            "This movie isnt has bad enough actors humor to to robert keep [UNK] the who suspense work and is raise intended the for audience their its lives difficult in to them follow but ups is something uninspired different and rent like it lynch past should live have it come most up be\n",
            "== Generating with temperature 1.5\n",
            "This movie film sucks has it terrible every acting time looked this like movie this was i made wrong any it sense was award a winning joke career no door the no funniest better one van that damme could ever even be possibly quite noted good in bad the special action effects\n",
            "391/391 [==============================] - 163s 415ms/step - loss: 4.3899\n",
            "Epoch 6/200\n",
            "105/391 [=======>......................] - ETA: 1:54 - loss: 4.3372"
          ]
        }
      ],
      "source": [
        "model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}